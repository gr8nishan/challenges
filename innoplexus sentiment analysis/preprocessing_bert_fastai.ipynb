{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')[:1]\n",
    "    \n",
    "    if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    elif file == '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin':\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.progress_apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.capitalize() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.capitalize()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.lower() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.lower()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.upper() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.upper()]\n",
    "            nb_known_words += vocab[word]\n",
    "        else:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def vocab_check_coverage(train, test):\n",
    "    df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    \n",
    "    vocab = build_vocab(df['text'])\n",
    "    print(\"Glove : \")\n",
    "    oov_glove = check_coverage(vocab, embed_glove)\n",
    "    oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
    "    print(\"Paragram : \")\n",
    "    oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "    oov_paragram = {\"oov_rate\": len(oov_paragram) / len(vocab), 'oov_words': oov_paragram}\n",
    "    print(\"FastText : \")\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    oov_fasttext = {\"oov_rate\": len(oov_fasttext) / len(vocab), 'oov_words': oov_fasttext}\n",
    "#     print(\"Google : \")\n",
    "#     oov_google = check_coverage(vocab, embed_google)\n",
    "#     oov_google = {\"oov_rate\": len(oov_google) / len(vocab), 'oov_words': oov_google}\n",
    "    \n",
    "    return oov_glove, oov_paragram, oov_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n",
      "Extracting Paragram embedding\n",
      "Extracting FastText embedding\n"
     ]
    }
   ],
   "source": [
    "glove = '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "paragram =  '../input/quoratextemb/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "wiki_news = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "google_path = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "print(\"Extracting GloVe embedding\")\n",
    "embed_glove = load_embed(glove)\n",
    "print(\"Extracting Paragram embedding\")\n",
    "embed_paragram = load_embed(paragram)\n",
    "print(\"Extracting FastText embedding\")\n",
    "embed_fasttext = load_embed(wiki_news)\n",
    "# print(\"Extracting GoogleNews embedding\")\n",
    "# embed_google = load_embed(google_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/innoplexusav/train.csv\")\n",
    "test_df  = pd.read_csv(\"../input/innoplexusav/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[['drug','text']]\n",
    "test = test_df[['drug','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "test['text'] = test['text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "\n",
    "train['text'] = train['text'].str.replace(r\"\\(.*?\\)\",\"\")\n",
    "test['text'] = test['text'].str.replace(r\"\\(.*?\\)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because','â€™': \"'\",',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not','â€“': '-','â€œ':'\"',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
    "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
    "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
    "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
    "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
    "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
    "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
    "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
    "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
    "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
    "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
    "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
    "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
    "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
    "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
    "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
    "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
    "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
    "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
    "    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
    "    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
    "    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
    "    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
    "    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n",
    "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
    "    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_case_words = {'tkis' : 'Tyrosine kinase inhibitors','blogthis':'blog','tnfα':'tumor necrosis factor',\n",
    "                  'jimc':'johannesburg international mail centre','iraes':'immune related adverse events',' ive ':' i have ',\n",
    "                  'pdl1':'programmed death ligand','pd1':'programmed death ligand','5asas':'aminosalicylates',' doc ':' doctor ','🙂':'good',\n",
    "                 ' cannot ': ' can not ',' spms ': ' secondary progressive multiple sclerosis ',' aes ': ' adverse events ',\n",
    "                 'saes':'serious adverse events','anti-cd20':'monoclonal antibodies',' mri ': ' magnetic resonance imaging ',\n",
    "                 ' re ':' are ',' dmt ': ' drug ',' jcv ': ' virus ',' chemo ': ' chemotherapy ',\n",
    "                 ' 75mgs ': ' 75 miligrams ','α4β7':'integrin',' pharmgkb ': ' pharmacogenomics knowledgebase ','\\x80\\x99l':'\"',\n",
    "                ' spms ':' secondary progressive multiple sclerosis ','azd9291':'osimertinib',' mymsteam ': 'multiple sclerosis support ',\n",
    "                 ' the ':' ',' an ':' ',' amultiple ':' multiple ',' nsclc ': ' non small cell lung cancer ',' meds ': ' medicines ',\n",
    "                 'thepowerofpoop':'the power of poop',' asas ':' aspirin ','egfr':'estimated glomerular filtration rate',\n",
    "                  ' alk ': ' lung cancer ',' kras mutation ':' mutation ',' imrt ': ' intensity modulated radiation therapy ',\n",
    "                ' ms ': ' multiple sclerosis ',' c797s ':' mutation cells ',' nhs ': ' national health service',\n",
    "                  'politicalspeak': 'political speak','newsspeak':'news speak','clinicaltrials':'clinical trials',\n",
    "                  'stillstannding':'still standing','naturalreader':'natural reader'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5279/5279 [00:04<00:00, 1216.88it/s]\n",
      "100%|██████████| 2924/2924 [00:02<00:00, 1037.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "test['text']  = test['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_punct.remove('-')\n",
    "#all_punct.remove('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # remove_diacritics don´t' ->  'don t'\n",
    "    #text = remove_diacritics(text)\n",
    "    return text\n",
    "\n",
    "def spacing_some_connect_words(text):\n",
    "    \"\"\"\n",
    "    'Whyare' -> 'Why are'\n",
    "    \"\"\"\n",
    "    ori = text\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])\n",
    "\n",
    "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
    "    text = text.replace(\"What sApp\", ' WhatsApp ')\n",
    "    text = remove_space(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_bad_case_words(text):\n",
    "    for bad_word in bad_case_words:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #text = remove_space(text)\n",
    "    text = clean_bad_case_words(text)\n",
    "    #text = spacing_some_connect_words(text)\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].apply(preprocess)\n",
    "test[\"text\"] = test[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "test['text'] = test['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "test['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "test['text'].replace({'  ':' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8203/8203 [00:00<00:00, 18028.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 85.82% of vocab\n",
      "Found embeddings for  99.06% of all text\n",
      "Paragram : \n",
      "Found embeddings for 86.18% of vocab\n",
      "Found embeddings for  99.08% of all text\n",
      "FastText : \n",
      "Found embeddings for 82.03% of vocab\n",
      "Found embeddings for  98.92% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove, oov_paragram, oov_fasttext = vocab_check_coverage(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ocrevus', 1854),\n",
       " ('entyvio', 1197),\n",
       " ('keytruda', 974),\n",
       " ('opdivo', 910),\n",
       " ('tagrisso', 761),\n",
       " ('pembrolizumab', 627),\n",
       " ('nivolumab', 620),\n",
       " ('tecfidera', 540),\n",
       " ('osimertinib', 440),\n",
       " ('ros1', 402),\n",
       " ('vedolizumab', 269),\n",
       " ('alectinib', 241),\n",
       " ('durvalumab', 214),\n",
       " ('atezolizumab', 208),\n",
       " ('dabrafenib', 206),\n",
       " ('uceris', 195),\n",
       " ('trametinib', 191),\n",
       " ('siponimod', 189),\n",
       " ('inflectra', 169),\n",
       " ('proctosigmoiditis', 160),\n",
       " ('mavenclad', 156),\n",
       " ('ceritinib', 144),\n",
       " ('tecentriq', 144),\n",
       " ('upadacitinib', 116),\n",
       " ('ibdsuperheroes', 114),\n",
       " ('brigatinib', 109),\n",
       " ('dcvax', 108),\n",
       " ('imfinzi', 94),\n",
       " ('ileorectal', 86),\n",
       " ('qbtx', 79),\n",
       " ('ozanimod', 79),\n",
       " ('catdander', 78),\n",
       " ('renflexis', 76),\n",
       " ('risankizumab', 70),\n",
       " ('baf312', 67),\n",
       " ('ipoop', 65),\n",
       " ('msers', 62),\n",
       " ('mekinist', 62),\n",
       " ('tafinlar', 60),\n",
       " ('filgotinib', 59),\n",
       " ('delzicol', 56),\n",
       " ('lorlatinib', 55),\n",
       " ('rociletinib', 54),\n",
       " ('zykadia', 53),\n",
       " ('gilotrif', 52),\n",
       " ('necitumumab', 51),\n",
       " ('jpouch', 49),\n",
       " ('serviceengland', 49),\n",
       " ('actrims', 46),\n",
       " ('zinbryta', 46)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_glove['oov_words'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reply posted for jesszidek . hi jess sorry to read about challenges you are having with your health . you mentioned lot in your post . just want to share some info on few of points . first , know you said that you are scared of humira . humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission . to reduce your level of fear it can help to learn more about your treatment option . you can learn more about some of your treatment options . to learn more view our understanding ibd medication brochure at : . if you would like to talk , contact help center at 888 - 694 - 8872 or at info @ crohnscolitisfoundation . org'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.replace('# ','')\n",
    "train['text'] = train['text'].str.replace(' - ','')\n",
    "train['text'] = train['text'].str.replace(' : ','')\n",
    "\n",
    "test['text'] = test['text'].str.replace('#','')\n",
    "test['text'] = test['text'].str.replace(' - ','')\n",
    "test['text'] = test['text'].str.replace(' : ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(data):\n",
    "    new=[]\n",
    "    for sentences in data:\n",
    "        yes = sentences.split(\". \")\n",
    "        new.append(yes) \n",
    "        \n",
    "    return new\n",
    "    \n",
    "train['new_text'] = splitting(train['text'])\n",
    "test['new_text'] = splitting(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning1(data):\n",
    "    new_text=[]\n",
    "    for sentences in data:\n",
    "        matching = [s for s in sentences if 'reply posted' not in s]\n",
    "        new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning1(train['new_text'])\n",
    "test['new_text'] = cleaning1(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning2(data):\n",
    "    new_text=[]\n",
    "    for sentences in data:\n",
    "        matching = [s for s in sentences if 'help center' not in s]\n",
    "        new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning2(train['new_text'])\n",
    "test['new_text'] = cleaning2(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning3(data):\n",
    "    new_text=[]\n",
    "    \n",
    "    for sentences in data:\n",
    "        if(len(sentences)>1):\n",
    "            matching = [s for s in sentences if len(s) >= 15]\n",
    "            new_text.append(matching)\n",
    "            \n",
    "        else:\n",
    "            matching = [s for s in sentences if len(s) >= 2]\n",
    "            new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning3(train['new_text'])\n",
    "test['new_text'] = cleaning3(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def don(d):\n",
    "    n=[]\n",
    "    for s in d:\n",
    "        res = \".\".join(s)\n",
    "        n.append(res)\n",
    "    return n       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = don(train['new_text'])\n",
    "test['text'] = don(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['drug','new_text'],axis=1,inplace=True)\n",
    "test.drop(['drug','new_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"drug\"] = train[\"drug\"].apply(preprocess)\n",
    "#drug_list = train['drug'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['text'],axis=1,inplace=True)\n",
    "test_df.drop(['text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_df,train,left_index=True, right_index=True,how='inner')\n",
    "test = pd.merge(test_df,test,left_index=True, right_index=True,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_2.csv',index=False)\n",
    "test.to_csv('test_2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
