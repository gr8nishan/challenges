{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom fastai.text import * \nfrom fastai.callbacks import *\nfrom pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom shutil import copyfile\nfrom sklearn.model_selection import train_test_split\n\n\n# nltk for preprocessing of text data\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\n# sklearn for preprocessing and machine learning models\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import AdaBoostClassifier,VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = Path('../input/')\ndaf = pd.read_csv(filepath/'train_2.csv')\ndaf = daf.fillna('be well')\ntestse = pd.read_csv(filepath/'test_2.csv')\n\ndaf['text'] = daf['text']+ ' ' +daf['drug']\ntestse['text'] = testse['text']+ ' ' +testse['drug']\ntestse2 = testse.copy()\n\ndf = daf[['drug','text','sentiment']]\ntests = testse[['drug','text']]","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df['text'].str.split().map(len)\nq = df[(df['word_count']>=200)]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tests['word_count'] = tests['text'].str.split().map(len)\np = tests[(tests['word_count']>=200)]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tests.drop(['word_count','drug'],axis=1,inplace=True)\ndf.drop(['word_count','drug'],axis=1,inplace=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def useful(n):\n    m =[]\n    \n    totals = list(n.index)\n    #print(totals)\n    for total in totals:\n        l = []\n        \n        t = n.text[total]\n        d = n.drug[total]\n        #print(d)\n    \n        v = t.split(\".\")\n    \n        for i in range(0,len(v)):\n            if(v[i].find(d)!=(-1)):\n                l.append(i)\n    \n        v = v[l[0]:len(v)]\n        s = '.'.join(v)\n        m.append(s)\n\n    return m        ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q['text'] = useful(q)\np['text'] = useful(p)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q.drop(['drug','sentiment','word_count'],axis=1,inplace=True)\np.drop(['drug','word_count'],axis=1,inplace=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df,q,right_index=True,left_index=True,how='outer')\ntests = pd.merge(tests,p,right_index=True,left_index=True,how='outer')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna('')\ndf['text'] = df['text_x']+df['text_y'] \n\ndf.drop(['text_x','text_y'],axis=1,inplace=True)\ndf2 = df.copy()\n\ntests = tests.fillna('')\ntests['text'] = tests['text_x']+tests['text_y'] \n\ntests.drop(['text_x','text_y'],axis=1,inplace=True)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path(os.path.abspath(os.curdir))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['neutral'] = df['sentiment'].apply(lambda x: 1. if x==2 else 0.)\ndf['positive'] = df['sentiment'].apply(lambda x: 1. if x==0 else 0.)\ndf['negative'] = df['sentiment'].apply(lambda x: 1. if x==1 else 0.)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df['text'].str.split().map(len)\ng = df[(df['word_count']>=400) & (df['sentiment']==2)]\ndf = pd.concat([df, g]).drop_duplicates(keep=False)\n\ndf.drop(['sentiment','word_count'],axis=1,inplace=True)\nlen(df)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"4142"},"metadata":{}}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = df[:int(len(df)*.999)]\nvalid = df[int(len(df)*.999):]\n\ntrain.to_csv('train.csv',index_label=False )\nvalid.to_csv('valid.csv',index_label=False )","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n\nconfig = Config(\n    testing=False,\n    bert_model_name=\"bert-large-uncased\",\n    max_lr=3e-5,\n    epochs=4,\n    use_fp16=True,\n    bs=16,\n    discriminative=False,\n    max_seq_len=256,\n)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_tok = BertTokenizer.from_pretrained(config.bert_model_name)","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 231508/231508 [00:00<00:00, 921841.16B/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FastAiBertTokenizer(BaseTokenizer): \n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs): \n         self._pretrained_tokenizer = tokenizer \n         self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n         return self \n    def tokenizer(self, t:str) -> List[str]: #Limits the maximum sequence length\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"] ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]=BOS):\n    \"\"\"Borrowed from fast.ai source\"\"\"\n    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n    if is1d(texts): texts = texts[:,None]\n    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n    for i in range(1,len(df.columns)):\n        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n    return text_col.values","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, val = [pd.read_csv(path / fname) for fname in [\"train.csv\", \"valid.csv\"]]\ntest = tests","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if config.testing:\n    train = train.head(64)\n    val = val.head(64)\n    test = test.head(64)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), \n                             pre_rules=[], post_rules=[])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = [\"negative\", \"neutral\", \"positive\"]\n","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"databunch = TextDataBunch.from_df(\".\", train, val, test,\n                   tokenizer=fastai_tokenizer,\n                   vocab=fastai_bert_vocab,\n                   include_bos=False,\n                   include_eos=False,\n                   text_cols=\"text\",\n                   label_cols=label_cols,\n                   bs=config.bs,\n                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n              )","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]\n\nclass BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=3)","execution_count":28,"outputs":[{"output_type":"stream","text":"100%|██████████| 1248501532/1248501532 [00:48<00:00, 25616258.17B/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = nn.BCEWithLogitsLoss()\nfrom fastai.callbacks import CSVLogger","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = Learner(databunch, bert_model,loss_func=loss_func, callback_fns=[partial(CSVLogger, append=True)])\nif config.use_fp16: learner = learner.to_fp16()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(4, 3e-5)","execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.507025</td>\n      <td>0.609925</td>\n      <td>03:53</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.491021</td>\n      <td>0.517374</td>\n      <td>04:24</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.428640</td>\n      <td>0.658327</td>\n      <td>04:06</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.323034</td>\n      <td>0.436515</td>\n      <td>04:30</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    #y = learner.get_preds(ds_type)[1].detach().cpu().numpy()\n    \n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    \n    return preds[reverse_sampler, :] ","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = get_preds_as_nparray(DatasetType.Test)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.argmax(preds, axis=-1)\ny_preds = np.zeros(preds.shape)\ny_preds[np.arange(preds.shape[0]), idx] = 1","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.DataFrame({1:preds[:,0],2:preds[:,1],0:preds[:,2]})\ns = pd.get_dummies(dataset).idxmax(1)\ndataseti = pd.DataFrame({'sentiment':s})","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.merge(testse,dataseti,left_index=True,right_index=True,how='inner')\nfinal.drop(['drug','text'],inplace=True,axis=1)\nfinal['sentiment'].value_counts()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"2    1868\n0     552\n1     504\nName: sentiment, dtype: int64"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}